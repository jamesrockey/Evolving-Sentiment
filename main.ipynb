{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolving sentiment in Crypto markets\n",
    "James Rockey (jrockey2)\n",
    "\n",
    "Kareem Benaissa (kareem2)\n",
    "\n",
    "### This project explores the relationship between cryptocurrency sentiment through tweets\n",
    "\n",
    "This is done in the following steps:\n",
    "\n",
    "1. Calculate sentiment of tweets with BERT model pretrained with financial sentiment\n",
    "\n",
    "Model: https://huggingface.co/ProsusAI/finbert\n",
    "\n",
    "Tweets dataset: https://www.kaggle.com/code/codeblogger/bitcoin-sentiment-analysis\n",
    "\n",
    "Tweets are preprocessed before they are fed into the model.\n",
    "\n",
    "The FinBERT model outputs percentage confidence in three following sentiment categories: ['positive', 'negative', 'neutral']\n",
    "\n",
    "2. After sentiment scores are calculated for every tweet, we compare the effect of sentiment on predictive power of LSTMs. \n",
    "\n",
    "For the purposes of this research we want to explore intraday sentiment. So, we collected tweets from 15 random days in February 2021 and March 2021, and for each of these days we calculated sentiment in 10 minute increments. We then considered different sized sequences of 10 minutes to predict the next 1 or 2 sequences. We compare LSTM's prediction of price, volume, and other features, using these features as a baseline, with its performance when sentiment and weighted average sentiment are included in trainging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Calculating Sentiment\n",
    "\n",
    "In our dataset, the 'text' column contains the content of the tweet. We preprocess this text and use our BERT model to calculate sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 48583 entries, 0 to 48582\n",
      "Data columns (total 13 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   user_name         48582 non-null  object\n",
      " 1   user_location     28273 non-null  object\n",
      " 2   user_description  45263 non-null  object\n",
      " 3   user_created      48583 non-null  object\n",
      " 4   user_followers    48583 non-null  int64 \n",
      " 5   user_friends      48583 non-null  int64 \n",
      " 6   user_favourites   48583 non-null  int64 \n",
      " 7   user_verified     48583 non-null  bool  \n",
      " 8   date              48583 non-null  object\n",
      " 9   text              48583 non-null  object\n",
      " 10  hashtags          38416 non-null  object\n",
      " 11  source            47685 non-null  object\n",
      " 12  is_retweet        48583 non-null  bool  \n",
      "dtypes: bool(2), int64(3), object(8)\n",
      "memory usage: 4.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('datasets/Bitcoin_tweets.csv', sep=',', header=0)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "MODEL = \"ProsusAI/finbert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions\n",
    "def get_labels(): \n",
    "    labels=[]\n",
    "\n",
    "    # This is for another BERT model that we tried:\n",
    "    # mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "    # with urllib.request.urlopen(mapping_link) as f:\n",
    "    #     html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    #     csvreader = csv.reader(html, delimiter='\\t')\n",
    "    # labels = [row[1] for row in csvreader if len(row) > 1]\n",
    "\n",
    "    # This is for the FinBERT model:\n",
    "    labels = ['positive', 'negative', 'neutral']\n",
    "    return labels\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    '''\n",
    "    Preprocess text (username and link placeholders)\n",
    "    '''    \n",
    "    new_text = []\n",
    "    for t in text.split(\" \"):\n",
    "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
    "        t = 'http' if t.startswith('http') else t\n",
    "        new_text.append(t)\n",
    "    return \" \".join(new_text)\n",
    "\n",
    "\n",
    "def process_tweet(text: str):\n",
    "    '''\n",
    "    Calculates sentiment scores for the given text\n",
    "    '''\n",
    "    text = preprocess(text)\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    scores = output[0][0].detach().numpy()\n",
    "    scores = softmax(scores)\n",
    "    return scores\n",
    "\n",
    "# Modified print_scores function to return values instead of printing\n",
    "def get_sentiment_scores(scores):\n",
    "    labels = get_labels()\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    return {labels[ranking[i]]: np.round(float(scores[ranking[i]]), 4) for i in range(scores.shape[0])}\n",
    "\n",
    "# Function to apply to each row\n",
    "def analyze_sentiment(row):\n",
    "    '''\n",
    "    Takes in row from dataframe and returns a series of sentiment scores\n",
    "    Used in df.apply()\n",
    "    '''\n",
    "    text = row['text']\n",
    "    scores = process_tweet(text)\n",
    "    labels = get_labels()\n",
    "    sentiment_scores = get_sentiment_scores(scores, labels)\n",
    "    print(pd.Series([sentiment_scores.get('positive', 0), \n",
    "                      sentiment_scores.get('neutral', 0), \n",
    "                      sentiment_scores.get('negative', 0)]))\n",
    "    return pd.Series([sentiment_scores.get('positive', 0), \n",
    "                      sentiment_scores.get('neutral', 0), \n",
    "                      sentiment_scores.get('negative', 0)])\n",
    "\n",
    "def print_scores(scores, labels):\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    for i in range(scores.shape[0]):\n",
    "        l = labels[ranking[i]]\n",
    "        s = scores[ranking[i]]\n",
    "        print(f\"{i+1}) {l} {np.round(float(s), 4)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example use of sentiment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) positive 0.9032\n",
      "2) neutral 0.0864\n",
      "3) negative 0.0103\n"
     ]
    }
   ],
   "source": [
    "text = '''I'm absolutely ecstatic about Bitcoin's remarkable \n",
    "performance and incredibly optimistic about its\n",
    " potential to revolutionize finance''' # example tweet\n",
    "\n",
    "scores = process_tweet(text)\n",
    "labels = get_labels()\n",
    "print_scores(scores, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply to dataset\n",
    "Note, this may take several hours. Data with sentiment scores already calculated is found in this file: sentiment_added_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[['positive', 'neutral', 'negative']] = df.apply(analyze_sentiment, axis=1)\n",
    "# df.to_csv('sentiment_added_data.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ____________________________________________________________________________\n",
    "# Step 2: Analyzing Sentiment\n",
    "\n",
    "Here, we explore how sentiment using the bert model effects LSTM prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate 10 minute sentiment scores for each day in dataset\n",
    "\n",
    "The sentiment score for a given day is found by taking an average of the ['positive', 'negative', 'neutral'] columns in our dataset during a 10-mintue interval. \n",
    "\n",
    "For each trading day, we group tweets into 10-minute intervals starting at 12:00:00 am until 11:59:59 pm. \n",
    "\n",
    "We also explore how the \"reach\" of a tweet affects daily sentiment by calculating a weighted average over the number of followers twitter user.\n",
    "\n",
    "If a 10-minute slice is missing tweets, we give each column equal weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The following functions calculate the weighted average \n",
    "and average sentiment scores for a group from the dataframe.\n",
    "\n",
    "Input: group: from the df.groupby() function\n",
    "\n",
    "Output: series of weighted average and average sentiment scores\n",
    "\n",
    "'''\n",
    "\n",
    "def weighted_average(group):\n",
    "    weighted_positive = (group['positive'] * group['user_followers']).sum()\n",
    "    weighted_neutral = (group['neutral'] * group['user_followers']).sum()\n",
    "    weighted_negative = (group['negative'] * group['user_followers']).sum()\n",
    "    total_followers = np.sum(group['user_followers'])\n",
    "    \n",
    "    if total_followers == 0:\n",
    "        return pd.Series({\n",
    "            'weighted_avg_positive': 1/3,\n",
    "            'weighted_avg_neutral': 1/3,\n",
    "            'weighted_avg_negative': 1/3\n",
    "        })\n",
    "    return pd.Series({\n",
    "        'weighted_avg_positive': weighted_positive / total_followers,\n",
    "        'weighted_avg_neutral': weighted_neutral / total_followers,\n",
    "        'weighted_avg_negative': weighted_negative / total_followers\n",
    "    })\n",
    "\n",
    "def average(group):\n",
    "    avg_positive = group['positive'].mean()\n",
    "    avg_neutral = group['neutral'].mean()\n",
    "    avg_negative = group['negative'].mean()\n",
    "\n",
    "    total_followers = np.sum(group['user_followers'])\n",
    "    \n",
    "    if total_followers == 0:\n",
    "        # print('INVALID TOTAL' + str(count))\n",
    "        return pd.Series({\n",
    "            'weighted_avg_positive': 1/3,\n",
    "            'weighted_avg_neutral': 1/3,\n",
    "            'weighted_avg_negative': 1/3\n",
    "        })\n",
    "    return pd.Series({\n",
    "        'avg_positive': avg_positive,\n",
    "        'avg_neutral': avg_neutral,\n",
    "        'avg_negative': avg_negative\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group Data into 10 minute intervals\n",
    "\n",
    "consider hour groups of data that have at least 1 tweet, and for each hour split into six 10 minute intervals\n",
    "\n",
    "For missing 10 minute intervals, put average value as default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# This function takes in a dataframe for a single day \n",
    "# and returns dataframe filtered into 10 minute sentiment intervals by the date\n",
    "# '''\n",
    "from datetime import datetime\n",
    "def process_day_data(day_df, interval_size='10T', default_values=None):\n",
    "    # Ensure 'date' column is in datetime format\n",
    "    day_df['date'] = pd.to_datetime(day_df['date'])\n",
    "\n",
    "    # Extract the date part as a string\n",
    "    day_str = day_df['date'].dt.date.iloc[0].isoformat()\n",
    "\n",
    "    # Identify hours with at least one tweet\n",
    "    active_hours = day_df['date'].dt.hour.unique()\n",
    "\n",
    "    # Create a DataFrame to hold the final result\n",
    "    processed_data = pd.DataFrame()\n",
    "\n",
    "    # Set default values for missing data points\n",
    "    default_values = default_values if default_values is not None else 1/3\n",
    "\n",
    "    # Iterate through each active hour and process data\n",
    "    for hour in active_hours:\n",
    "        # Create a datetime object for the start of the hour\n",
    "        start_time_str = f\"{day_str} {hour:02d}:00:00\"\n",
    "        start_time = datetime.fromisoformat(start_time_str)\n",
    "        end_time = start_time + pd.Timedelta(hours=1) - pd.Timedelta(seconds=1)\n",
    "\n",
    "\n",
    "        # Create date range for this hour\n",
    "        date_range = pd.date_range(start=start_time, end=end_time, freq=interval_size)\n",
    "\n",
    "        # Filter the DataFrame for the current hour and group by interval\n",
    "        hour_df = day_df[(day_df['date'] >= start_time) & (day_df['date'] <= end_time)]\n",
    "        grouped_data = hour_df.groupby(pd.Grouper(key='date', freq=interval_size)).apply(average).reset_index()\n",
    "\n",
    "        # Reindex to ensure all intervals are present\n",
    "        grouped_data.set_index('date', inplace=True)\n",
    "        grouped_data = grouped_data.reindex(date_range)\n",
    "\n",
    "        # Fill missing data with default values\n",
    "        grouped_data.fillna(default_values, inplace=True)\n",
    "\n",
    "        # Add the processed data for this hour to the final DataFrame\n",
    "        processed_data = pd.concat([processed_data, grouped_data.reset_index()])\n",
    "\n",
    "    # Reset index and rename the date column\n",
    "    processed_data.reset_index(drop=True, inplace=True)\n",
    "    processed_data.rename(columns={'index': 'date'}, inplace=True)\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter sentiment grouped into days, and then into 10 minute intervals\n",
    "\n",
    "Reformat data and fill in for any gaps in sentiment score with average value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment_df = pd.read_csv('datasets/refactored_sentiment_added_data_and_date.csv')\n",
    "# sentiment_df['date'] = pd.to_datetime(sentiment_df['date'])\n",
    "# # sentiment_df.info()\n",
    "# day_tweets_df = sentiment_df.groupby(sentiment_df['date'].dt.day)\n",
    "\n",
    "\n",
    "# x, y = [], []\n",
    "# idx = 0\n",
    "# for day, day_df in day_tweets_df:\n",
    "#     hour_df = day_df.groupby(day_df['date'].dt.hour)\n",
    "#     for hour, hour_df in hour_df:\n",
    "#         x.append(idx)\n",
    "\n",
    "#         y.append(hour_df['text'].count())\n",
    "#         idx += 1\n",
    "\n",
    "# # plot scatter plot\n",
    "# plt.scatter(x, y)\n",
    "# plt.xlabel('Hour')\n",
    "# plt.ylabel('Number of Tweets')\n",
    "# plt.title('Number of Tweets per Hour')\n",
    "# plt.show()\n",
    "\n",
    "# print(len(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df = pd.read_csv('datasets/refactored_sentiment_added_data_and_date.csv')\n",
    "\n",
    "# information to keep: user_followers, date, positive, neutral, negative\n",
    "sentiment_df = sentiment_df[['user_followers', 'date', 'positive', 'neutral', 'negative']]\n",
    "sentiment_df['date'] = pd.to_datetime(sentiment_df['date'])\n",
    "\n",
    "# group by days\n",
    "day_tweets_df = sentiment_df.groupby(sentiment_df['date'].dt.date)\n",
    "\n",
    "day_data = []\n",
    "\n",
    "\n",
    "for day, group_data in day_tweets_df:\n",
    "    data = process_day_data(group_data) # process data for each day (fill in missing interval data)\n",
    "    day_data.append(data)\n",
    "\n",
    "# print(day_data.keys())\n",
    "day_tweets_df = pd.concat(day_data, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bitcoin Price grouped into 10 minute intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1752 entries, 0 to 1751\n",
      "Data columns (total 12 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   date          1752 non-null   datetime64[ns]\n",
      " 1   avg_positive  1752 non-null   float64       \n",
      " 2   avg_neutral   1752 non-null   float64       \n",
      " 3   avg_negative  1752 non-null   float64       \n",
      " 4   unix          1752 non-null   int64         \n",
      " 5   symbol        1752 non-null   object        \n",
      " 6   open          1752 non-null   float64       \n",
      " 7   high          1752 non-null   float64       \n",
      " 8   low           1752 non-null   float64       \n",
      " 9   close         1752 non-null   float64       \n",
      " 10  Volume BTC    1752 non-null   float64       \n",
      " 11  Volume USD    1752 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(9), int64(1), object(1)\n",
      "memory usage: 177.9+ KB\n"
     ]
    }
   ],
   "source": [
    "btc_minute_df = pd.read_csv('datasets/BTC-2021min.csv', sep=',', header=0)\n",
    "btc_minute_df['date'] = pd.to_datetime(btc_minute_df['date'])\n",
    "\n",
    "combined_data = pd.merge(day_tweets_df, btc_minute_df, on='date', how='inner')\n",
    "\n",
    "combined_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes in pandas dataframe and returns nonoverlapping sequences\n",
    "\n",
    "data: np ndarray of the values of a set of columns\n",
    "\n",
    "lookback: number of rows to look back on\n",
    "m: number of rows after lookback to include in sequence\n",
    "\n",
    "X: where len(X) = lookback \n",
    "y: where len(y) = m\n",
    "'''\n",
    "\n",
    "def create_sequences(data, lookback: int, m = 1, output_features = [1, 2, 3]):\n",
    "    X, y = [], []\n",
    "    for i in range(0, len(data) - lookback, lookback + m):\n",
    "        # for given window, get only the features columns\n",
    "        X_seq = data[i:(i+lookback)] # start at first sequence\n",
    "        y_seq = data[i+lookback:i+lookback+m, output_features]\n",
    "\n",
    "        X.append(X_seq)\n",
    "        y.append(y_seq)\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose features below to include in training LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1752 entries, 0 to 1751\n",
      "Data columns (total 12 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   date          1752 non-null   datetime64[ns]\n",
      " 1   avg_positive  1752 non-null   float64       \n",
      " 2   avg_neutral   1752 non-null   float64       \n",
      " 3   avg_negative  1752 non-null   float64       \n",
      " 4   unix          1752 non-null   int64         \n",
      " 5   symbol        1752 non-null   object        \n",
      " 6   open          1752 non-null   float64       \n",
      " 7   high          1752 non-null   float64       \n",
      " 8   low           1752 non-null   float64       \n",
      " 9   close         1752 non-null   float64       \n",
      " 10  Volume BTC    1752 non-null   float64       \n",
      " 11  Volume USD    1752 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(9), int64(1), object(1)\n",
      "memory usage: 177.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(combined_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1752 entries, 0 to 1751\n",
      "Data columns (total 12 columns):\n",
      " #   Column        Non-Null Count  Dtype         \n",
      "---  ------        --------------  -----         \n",
      " 0   date          1752 non-null   datetime64[ns]\n",
      " 1   avg_positive  1752 non-null   float64       \n",
      " 2   avg_neutral   1752 non-null   float64       \n",
      " 3   avg_negative  1752 non-null   float64       \n",
      " 4   unix          1752 non-null   int64         \n",
      " 5   symbol        1752 non-null   object        \n",
      " 6   open          1752 non-null   float64       \n",
      " 7   high          1752 non-null   float64       \n",
      " 8   low           1752 non-null   float64       \n",
      " 9   close         1752 non-null   float64       \n",
      " 10  Volume BTC    1752 non-null   float64       \n",
      " 11  Volume USD    1752 non-null   float64       \n",
      "dtypes: datetime64[ns](1), float64(9), int64(1), object(1)\n",
      "memory usage: 177.9+ KB\n",
      "None\n",
      "15\n",
      "(225, 5, 2) (225, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "num_days = combined_data.groupby(combined_data['date'].dt.date).ngroups\n",
    "\n",
    "\n",
    "# TODO: change to your desired features\n",
    "\n",
    "features = ['close', 'avg_negative']       # 'weighted_avg_positive', 'weighted_avg_neutral', 'weighted_avg_negative', \n",
    "output_features = [0] # only predict close\n",
    "lookback_size = 5 # \n",
    "prediction_size = 1\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "print(combined_data.info())\n",
    "scaled_features = scaler.fit_transform(combined_data[features].values)\n",
    "\n",
    "\n",
    "training_data = []\n",
    "testing_data = []\n",
    "\n",
    "for i in range(0, num_days):\n",
    "    # get data for that day\n",
    "    num_rows = scaled_features.shape[0] // num_days\n",
    "    day_data = scaled_features[i*num_rows:(i+1)*num_rows]\n",
    "\n",
    "    X, y = create_sequences(day_data, lookback_size, prediction_size, output_features)\n",
    "\n",
    "    training_data.append((X[:int(len(X)*0.8)], y[:int(len(y)*0.8)]))\n",
    "    testing_data.append((X[int(len(X)*0.8):], y[int(len(y)*0.8):]))\n",
    "\n",
    "print(len(training_data))\n",
    "\n",
    "X_train = np.concatenate([data[0] for data in training_data])\n",
    "y_train = np.concatenate([data[1] for data in training_data])\n",
    "X_test = np.concatenate([data[0] for data in testing_data])\n",
    "y_test = np.concatenate([data[1] for data in testing_data]) \n",
    "\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(lookback_size, len(features))))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "8/8 [==============================] - 2s 55ms/step - loss: 0.2459 - val_loss: 0.1626\n",
      "Epoch 2/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0899 - val_loss: 0.0257\n",
      "Epoch 3/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0212 - val_loss: 0.0260\n",
      "Epoch 4/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0207 - val_loss: 0.0105\n",
      "Epoch 5/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0146 - val_loss: 0.0102\n",
      "Epoch 6/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0093 - val_loss: 0.0067\n",
      "Epoch 7/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0075 - val_loss: 0.0024\n",
      "Epoch 8/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0049 - val_loss: 0.0015\n",
      "Epoch 9/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0042 - val_loss: 0.0013\n",
      "Epoch 10/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0041 - val_loss: 9.6715e-04\n",
      "Epoch 11/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0041 - val_loss: 9.2681e-04\n",
      "Epoch 12/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0041 - val_loss: 0.0012\n",
      "Epoch 13/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0039 - val_loss: 8.6898e-04\n",
      "Epoch 14/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0039 - val_loss: 0.0011\n",
      "Epoch 15/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0040 - val_loss: 8.0633e-04\n",
      "Epoch 16/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0039 - val_loss: 8.0726e-04\n",
      "Epoch 17/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0039 - val_loss: 7.6159e-04\n",
      "Epoch 18/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0039 - val_loss: 7.5373e-04\n",
      "Epoch 19/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0038 - val_loss: 7.7625e-04\n",
      "Epoch 20/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0040 - val_loss: 8.2877e-04\n",
      "Epoch 21/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 6.9325e-04\n",
      "Epoch 22/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 7.1816e-04\n",
      "Epoch 23/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 6.7213e-04\n",
      "Epoch 24/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 6.6265e-04\n",
      "Epoch 25/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 6.3290e-04\n",
      "Epoch 26/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 0.0011\n",
      "Epoch 27/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0046 - val_loss: 6.3317e-04\n",
      "Epoch 28/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 6.5101e-04\n",
      "Epoch 29/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 7.9487e-04\n",
      "Epoch 30/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0037 - val_loss: 5.5721e-04\n",
      "Epoch 31/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 5.9321e-04\n",
      "Epoch 32/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 5.2423e-04\n",
      "Epoch 33/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 6.3474e-04\n",
      "Epoch 34/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 5.5125e-04\n",
      "Epoch 35/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 5.1808e-04\n",
      "Epoch 36/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 5.4954e-04\n",
      "Epoch 37/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 4.8630e-04\n",
      "Epoch 38/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 5.6612e-04\n",
      "Epoch 39/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 5.1256e-04\n",
      "Epoch 40/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 5.2161e-04\n",
      "Epoch 41/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 4.9018e-04\n",
      "Epoch 42/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0034 - val_loss: 4.8039e-04\n",
      "Epoch 43/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0034 - val_loss: 5.1461e-04\n",
      "Epoch 44/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0034 - val_loss: 4.6809e-04\n",
      "Epoch 45/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0034 - val_loss: 5.3873e-04\n",
      "Epoch 46/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 4.7220e-04\n",
      "Epoch 47/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0035 - val_loss: 4.9017e-04\n",
      "Epoch 48/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0039 - val_loss: 6.9848e-04\n",
      "Epoch 49/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 4.5806e-04\n",
      "Epoch 50/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 4.7859e-04\n",
      "Epoch 51/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0034 - val_loss: 5.1029e-04\n",
      "Epoch 52/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0033 - val_loss: 4.5656e-04\n",
      "Epoch 53/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0034 - val_loss: 0.0012\n",
      "Epoch 54/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0050 - val_loss: 0.0015\n",
      "Epoch 55/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0045 - val_loss: 0.0013\n",
      "Epoch 56/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0038 - val_loss: 6.2259e-04\n",
      "Epoch 57/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 5.3737e-04\n",
      "Epoch 58/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0036 - val_loss: 5.1076e-04\n",
      "Epoch 59/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0034 - val_loss: 4.8840e-04\n",
      "Epoch 60/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0034 - val_loss: 4.5571e-04\n",
      "Epoch 61/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0033 - val_loss: 4.5474e-04\n",
      "Epoch 62/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0033 - val_loss: 4.9593e-04\n",
      "Epoch 63/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0033 - val_loss: 4.6810e-04\n",
      "Epoch 64/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0033 - val_loss: 4.4120e-04\n",
      "Epoch 65/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0033 - val_loss: 4.5048e-04\n",
      "Epoch 66/100\n",
      "8/8 [==============================] - 0s 9ms/step - loss: 0.0033 - val_loss: 6.1566e-04\n",
      "Epoch 67/100\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0033 - val_loss: 4.3218e-04\n",
      "Epoch 68/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0033 - val_loss: 6.6015e-04\n",
      "Epoch 69/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0036 - val_loss: 4.6932e-04\n",
      "Epoch 70/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0033 - val_loss: 5.1382e-04\n",
      "Epoch 71/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0033 - val_loss: 4.3709e-04\n",
      "Epoch 72/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0032 - val_loss: 4.6711e-04\n",
      "Epoch 73/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0032 - val_loss: 4.2971e-04\n",
      "Epoch 74/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0033 - val_loss: 5.8448e-04\n",
      "Epoch 75/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0033 - val_loss: 4.8260e-04\n",
      "Epoch 76/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0035 - val_loss: 7.4520e-04\n",
      "Epoch 77/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0034 - val_loss: 4.6387e-04\n",
      "Epoch 78/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0032 - val_loss: 4.9840e-04\n",
      "Epoch 79/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0032 - val_loss: 5.4571e-04\n",
      "Epoch 80/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0033 - val_loss: 4.6694e-04\n",
      "Epoch 81/100\n",
      "8/8 [==============================] - 0s 5ms/step - loss: 0.0032 - val_loss: 8.9876e-04\n",
      "Epoch 82/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0037 - val_loss: 6.1636e-04\n",
      "Epoch 83/100\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 0.0033 - val_loss: 9.9259e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x17cb2df10>"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# Training the model\n",
    "model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), callbacks=[early_stopping], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step - loss: 5.0719e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0005071906489320099"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with sentiment included performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step - loss: 4.1852e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00041852155118249357"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)\n",
    "# 0.00015919399447739124 < 0.00015837745741009712\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step - loss: 5.4802e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0005480234976857901"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step - loss: 9.9259e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.000992588116787374"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
